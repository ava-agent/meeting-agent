// æœåŠ¡ç«¯ AI ç”Ÿæˆè·¯ç”± (GLM API ä»£ç†)
// API key ä¿å­˜åœ¨æœåŠ¡ç«¯ç¯å¢ƒå˜é‡ä¸­ï¼Œä¸æš´éœ²ç»™æµè§ˆå™¨

import { NextRequest, NextResponse } from 'next/server'
import { PROMPT_TEMPLATES, getSystemPrompt } from '@/services/ai/prompt-templates'
import type { GenerationType } from '@/types/ai'

const GLM_API_BASE = 'https://open.bigmodel.cn/api/paas/v4/chat/completions'

interface GenerateRequest {
  type: GenerationType
  meetingData: {
    title: string
    date?: string
    location?: string
    description?: string
    attendees?: string
    budget?: string
    type?: string
    duration?: string
  }
}

function getMockContent(type: GenerationType): string {
  const mocks: Record<GenerationType, string> = {
    agenda: `ğŸ“‹ ä¼šè®®è®®ç¨‹ï¼ˆæ¼”ç¤ºå†…å®¹ï¼‰\n\n1. ã€å¼€å¹• - 30åˆ†é’Ÿã€‘\n   - ä¸»æŒäººå¼€åœº\n   - é¢†å¯¼è‡´è¾\n\n2. ã€ä¸»é¢˜åˆ†äº« - 60åˆ†é’Ÿã€‘\n   - æ ¸å¿ƒå†…å®¹\n   - æ¡ˆä¾‹åˆ†æ\n\n3. ã€äº’åŠ¨è®¨è®º - 45åˆ†é’Ÿã€‘\n   - åˆ†ç»„è®¨è®º\n   - Q&A\n\n4. ã€æ€»ç»“ - 15åˆ†é’Ÿã€‘\n   - ä¼šè®®æ€»ç»“\n   - è¡ŒåŠ¨è®¡åˆ’\n\nâš ï¸ è¯·é…ç½® GLM_API_KEY è·å– AI çœŸå®ç”Ÿæˆå†…å®¹`,
    speech: `ğŸ¤ å¼€åœºè‡´è¾ï¼ˆæ¼”ç¤ºå†…å®¹ï¼‰\n\nå°Šæ•¬çš„å„ä½æ¥å®¾ï¼š\n\néå¸¸è£å¹¸ä¸»æŒæœ¬æ¬¡ä¼šè®®ã€‚ä»Šå¤©æˆ‘ä»¬æ±‡èšä¸€å ‚ï¼Œå…±åŒæ¢è®¨é‡è¦è®®é¢˜ã€‚\n\næœ¬æ¬¡ä¼šè®®çš„æ ¸å¿ƒç›®æ ‡æ˜¯æ¨åŠ¨äº¤æµä¸åˆä½œï¼Œå…±åŒè¿æ¥æ–°çš„å‘å±•æœºé‡ã€‚\n\né¢„ç¥ä¼šè®®åœ†æ»¡æˆåŠŸï¼\n\nâš ï¸ è¯·é…ç½® GLM_API_KEY è·å– AI çœŸå®ç”Ÿæˆå†…å®¹`,
    poster: `ğŸ¨ æµ·æŠ¥è®¾è®¡æ–¹æ¡ˆï¼ˆæ¼”ç¤ºå†…å®¹ï¼‰\n\nè®¾è®¡é£æ ¼ï¼šç°ä»£å•†åŠ¡\nä¸»è‰²è°ƒï¼šè“è‰²æ¸å˜\n\næ ¸å¿ƒå…ƒç´ ï¼š\n- ä¼šè®®ä¸»é¢˜æ ‡é¢˜\n- æ—¶é—´åœ°ç‚¹ä¿¡æ¯\n- ä¸»åŠæ–¹ Logo\n- äºŒç»´ç \n\nâš ï¸ è¯·é…ç½® GLM_API_KEY è·å– AI çœŸå®ç”Ÿæˆå†…å®¹`,
    gifts: `ğŸ ä¼´æ‰‹ç¤¼æ¨èï¼ˆæ¼”ç¤ºå†…å®¹ï¼‰\n\næ–¹æ¡ˆä¸€ï¼šå•†åŠ¡ç¤¼å“å¥—è£… - Â¥38/ä»½\næ–¹æ¡ˆäºŒï¼šä¿æ¸©æ¯ç¤¼ç›’ - Â¥45/ä»½\næ–¹æ¡ˆä¸‰ï¼šå……ç”µå®å¥—è£… - Â¥52/ä»½\n\nâš ï¸ è¯·é…ç½® GLM_API_KEY è·å– AI çœŸå®ç”Ÿæˆå†…å®¹`
  }
  return mocks[type] || 'æ¼”ç¤ºå†…å®¹'
}

export async function POST(request: NextRequest) {
  try {
    const body: GenerateRequest = await request.json()
    const { type, meetingData } = body

    if (!type || !meetingData) {
      return NextResponse.json(
        { success: false, error: 'ç¼ºå°‘å¿…è¦å‚æ•°: type å’Œ meetingData' },
        { status: 400 }
      )
    }

    const validTypes: GenerationType[] = ['agenda', 'speech', 'poster', 'gifts']
    if (!validTypes.includes(type)) {
      return NextResponse.json(
        { success: false, error: `ä¸æ”¯æŒçš„ç”Ÿæˆç±»å‹: ${type}` },
        { status: 400 }
      )
    }

    const apiKey = process.env.GLM_API_KEY
    if (!apiKey) {
      // æ—  API key æ—¶è¿”å›æ¼”ç¤ºå†…å®¹
      return NextResponse.json({
        success: true,
        content: getMockContent(type),
        usage: { prompt_tokens: 0, completion_tokens: 0, total_tokens: 0 },
        mock: true
      })
    }

    const template = PROMPT_TEMPLATES[type.toUpperCase() as keyof typeof PROMPT_TEMPLATES]
    const userPrompt = template(meetingData)
    const systemPrompt = getSystemPrompt(type)

    const glmResponse = await fetch(GLM_API_BASE, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${apiKey}`,
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        model: 'glm-4-flash',
        messages: [
          { role: 'system', content: systemPrompt },
          { role: 'user', content: userPrompt }
        ],
        temperature: 0.7,
        top_p: 0.9,
        max_tokens: 2000
      }),
      signal: AbortSignal.timeout(60000)
    })

    if (!glmResponse.ok) {
      const errorData = await glmResponse.json().catch(() => ({}))
      const errorMessage = (errorData as { error?: { message?: string } }).error?.message || glmResponse.statusText
      return NextResponse.json(
        { success: false, error: `GLM API é”™è¯¯: ${errorMessage}` },
        { status: glmResponse.status }
      )
    }

    const glmData = await glmResponse.json() as {
      choices: Array<{ message: { content: string } }>
      usage: { prompt_tokens: number; completion_tokens: number; total_tokens: number }
    }
    const content = glmData.choices[0]?.message?.content || ''

    return NextResponse.json({
      success: true,
      content,
      usage: glmData.usage
    })
  } catch (error) {
    console.error('AI ç”Ÿæˆå¤±è´¥:', error)
    return NextResponse.json(
      { success: false, error: error instanceof Error ? error.message : 'AI ç”Ÿæˆå¤±è´¥' },
      { status: 500 }
    )
  }
}
